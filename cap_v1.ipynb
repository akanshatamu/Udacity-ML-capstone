{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World!\n"
     ]
    }
   ],
   "source": [
    "print 'Hello World!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ">>>>>>>>>>>>>>>>>>Definition<<<<<<<<<<<<<<<<\n",
    "User clickstream data and information about a group of hotels is given. The users are classified into a set of classes.\n",
    "The classes are, Backpackers, Family, Couple. The objective is to predict the segment for a new customer using a\n",
    "classifier. The task is to explore different classifiers and then select the best one for predictions. Cross-validation\n",
    "is used to test the models by determining the F1 score.GraidCV and RandomCV are used to determine optimum\n",
    "hyper parameters for the classifiers.\n",
    ">>Data<<\n",
    "Data is given in the form of two tables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The exercise is done  based on the following steps, <Step-1> Data mining and extraction:- This step involves reading the train_search.csv and hotel.csv and converting the data into a single table, globalTrain.csv. This process is executed using \"join\" using python pandas. However, here due to the size of data, I used mysql to generate the table. <Step-2> Identifying features and target in the test and training data. The features are extracted from the globalTrain and the target is the \"Segment\". This step involves the python pandas library and the following dataframes are created: X_train (features of trainigng data), y_train (target of training data), X_test (features of test dat i.e. evaluation.csv) and y_test (target of test data i.e. our final objective, the y_test is generated at the end). <Step-3> Performs cross-validation based F1 score tests and build the classifier models. Then fit the training data in the classifier. <Step-4> Predict y_test from the classifier fits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read train_search.csv and store the data in a data frame named, \"searchData\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akansha/anaconda2/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# Tell iPython to include plots inline in the notebook\n",
    "%matplotlib inline\n",
    "# Read dataset\n",
    "searchData = pd.read_csv(\"train_search.csv\")\n",
    "#print \"Dataset has {} rows, {} columns\".format(*searchData.shape)\n",
    "#print searchData.head() # print the first 5 rows\n",
    "print \"Successfull!!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read Hotel.csv and store the data in a data frame named, \"hotelData\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfull!!\n"
     ]
    }
   ],
   "source": [
    "# Read dataset\n",
    "hotelData = pd.read_csv(\"Hotel.csv\")\n",
    "#print \"Dataset has {} rows, {} columns\".format(*hotelData.shape)\n",
    "#print hotelData.head() # print the first 5 rows\n",
    "print \"Successfull!!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now join the two data frames based on \"HotelCOde\" and develop a single data frame \"globalData\". in this process also create a csv file, \"globalTrain.csv\". Due to the size of searchData and computational limitations on my laptop I did not execute this join, rather used mysql to execute the join."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#globalData = searchData.join(hotelData, on='HotelCode')\n",
    "#print \"Dataset has {} rows, {} columns\".format(*globalData.shape)\n",
    "#print globalData.head() # print the first 5 rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfull!!\n"
     ]
    }
   ],
   "source": [
    "globalData = pd.read_csv(\"globalTrain.csv\")\n",
    "#print globalData.head() # print the first 5 rows\n",
    "globalData=globalData.rename(columns = {' HotelCode':'HotelCode'})\n",
    "globalData=globalData.rename(columns = {' Age':'Age'})\n",
    "globalData=globalData.rename(columns = {' Gender':'Gender'})\n",
    "globalData=globalData.rename(columns = {' Number of Rooms':'Number of Rooms'})\n",
    "globalData=globalData.rename(columns = {' Check in date':'Check in date'})\n",
    "globalData=globalData.rename(columns = {' Check Out Date':'Check Out Date'})\n",
    "globalData=globalData.rename(columns = {' Seen Price':'Seen Price'})\n",
    "globalData=globalData.rename(columns = {' isClicked':'isClicked'})\n",
    "globalData=globalData.rename(columns = {' isBooked':'isBooked'})\n",
    "globalData=globalData.rename(columns = {' Segment':'Segment'})\n",
    "#print globalData.dtypes;\n",
    "globalData['Booking Date'] =  pd.to_datetime(globalData['Booking Date'])\n",
    "globalData['Check in date'] =  pd.to_datetime(globalData['Check in date'])\n",
    "globalData['Check Out Date'] =  pd.to_datetime(globalData['Check Out Date'])\n",
    "globalData['isClicked'] =  globalData['isClicked'].astype(str)\n",
    "globalData['isBooked'] =  globalData['isBooked'].astype(str)\n",
    "\n",
    "globalData['Stay Period'] = (globalData['Check Out Date'] - globalData['Check in date'])/np.timedelta64(1, 'D');\n",
    "globalData['Travel Gap'] = (globalData['Check in date'] - globalData['Booking Date'])/np.timedelta64(1, 'D');\n",
    "#print globalData.dtypes;\n",
    "#print \"Dataset has {} rows, {} columns\".format(*globalData.shape)\n",
    "#print globalData.head() # print the first 5 rows\n",
    "print \"Successfull!!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the data, it is evident that the segment depends on,\n",
    "1> Age, 2> Gender, 3> Number of rooms, 4> Seen price, 5> isCLicked, 6> isBooked, 7> Star Rating, 8> Trip Adviser Rating, 9> Stay period (Difference in days between Check out date and Check in date), 10 > Travel Gap  (Difference in days between booking date and check in date).\n",
    "Therefore, there are 10 features as mentioned above. Some of the features have a string/object value, convert those to discretes ones an zeros. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akansha/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/akansha/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/akansha/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/akansha/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successful!!\n"
     ]
    }
   ],
   "source": [
    "#Now separate training data\n",
    "X_train=globalData.iloc[:,[3,4,5,8,9,10,16,17,18,19]];\n",
    "#X_train=globalData.iloc[:,[3]];\n",
    "X_train['Gender'] = X_train['Gender'].replace(['male', 'female'], [1, 0])\n",
    "X_train['isClicked'] = X_train['isClicked'].replace(['True', 'False'], [1, 0])\n",
    "X_train['isBooked'] = X_train['isBooked'].replace(['True', 'False'], [1, 0])\n",
    "X_train[X_train < 0] = 0\n",
    "#print X_train.head();\n",
    "#print X_train.dtypes;\n",
    "#print X_train['Travel Gap'];\n",
    "print \"Successful!!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import clustering modules\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GMM\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  9.99999999e-01   1.05329442e-09   7.57287566e-11   1.22392571e-12\n",
      "   2.25296960e-13   8.80978053e-14   8.45287761e-14   6.86137409e-14\n",
      "   4.96749658e-14   1.11425796e-14]\n"
     ]
    }
   ],
   "source": [
    "pca = PCA(n_components=10)\n",
    "pca.fit(X_train)\n",
    "# Print the components and the amount of variance in the data contained in each dimension\n",
    "#print pca.components_\n",
    "print pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "n_components=3 invalid for n_features=1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-a0c4d1aedbb0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# TODO: First we reduce the data to two dimensions using PCA to capture variation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mpca\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPCA\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mreduced_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpca\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[0mreduced_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m#print upto 10 elements\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[0mpca\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcomponents_\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/akansha/anaconda2/lib/python2.7/site-packages/sklearn/decomposition/pca.pyc\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    239\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    240\u001b[0m         \"\"\"\n\u001b[1;32m--> 241\u001b[1;33m         \u001b[0mU\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mV\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    242\u001b[0m         \u001b[0mU\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mU\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_components_\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    243\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/akansha/anaconda2/lib/python2.7/site-packages/sklearn/decomposition/pca.pyc\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    291\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[0mn_components\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[0mn_features\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    292\u001b[0m             raise ValueError(\"n_components=%r invalid for n_features=%d\"\n\u001b[1;32m--> 293\u001b[1;33m                              % (n_components, n_features))\n\u001b[0m\u001b[0;32m    294\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    295\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mn_components\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m1.0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: n_components=3 invalid for n_features=1"
     ]
    }
   ],
   "source": [
    "# TODO: First we reduce the data to two dimensions using PCA to capture variation\n",
    "pca = PCA(n_components=3)\n",
    "reduced_data = pca.fit_transform(X_train)\n",
    "print reduced_data[:10] #print upto 10 elements\n",
    "print pca.components_\n",
    "print pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Implement your clustering algorithm here, and fit it to the reduced data for visualizati\n",
    "# The visualizer below assumes your clustering object is named ’clusters’\n",
    "clusters = KMeans(init='k-means++', n_clusters=3, n_init=10)\n",
    "clusters.fit(reduced_data)\n",
    "#clusters = ?\n",
    "print clusters\n",
    "KMeans(copy_x=True, init='k-means++', max_iter=300, n_clusters=3, n_init=10,\n",
    "n_jobs=1, precompute_distances='auto', random_state=None, tol=0.0001,\n",
    "verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfull!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akansha/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "#Now separate training data - Target\n",
    "y_train=globalData.iloc[:,[11]];\n",
    "y_train['Segment'] = y_train['Segment'].replace(['backpacker', 'couple','family'], [1, 3, 2])\n",
    "#print y_train.head();\n",
    "#print y_train.dtypes;\n",
    "print \"Successfull!!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best way to test a classifier model is cross-validation. In cross-validation an input set is brocken into training and testing set. The testting set is used to test the model built using the training set. And then the F1 score is computed. The next block of code is used to split the input data randomly into traing set and testing set. After the block is executed a set of training and test features and target are available for cross validation. The variables generated are X_train_cv, y_train_cv, X_test_cv and y_test_cv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 122848 samples\n",
      "Test set: 40000 samples\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "# First, decide how many training vs test samples you want\n",
    "num_test = 40000\n",
    "X_train_cv, X_test_cv, y_train_cv, y_test_cv = train_test_split(X_train, y_train, test_size=num_test)\n",
    "print \"Training set: {} samples\".format(X_train_cv.shape[0])\n",
    "print \"Test set: {} samples\".format(X_test_cv.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next block of code is a method that fits a training data into an inputted classifier and also returns and prints the time it takes to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "def train_classifier(clf, X_train, y_train):\n",
    "    print \"Training {}...\".format(clf.__class__.__name__)\n",
    "    start = time.time()\n",
    "    clf.fit(X_train, y_train)\n",
    "    end = time.time()\n",
    "    print \"Done!\\nTraining time (secs): {:.3f}\".format(end - start)\n",
    "    return (end - start)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A decision tree classifier is initiated and a classifier model is built and fittedwith the cross-validation training features and target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training RandomForestClassifier...\n",
      "Done!\n",
      "Training time (secs): 5.353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akansha/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:9: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5.352785110473633"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit model to training data\n",
    "#Best classifier\n",
    "#clf = DecisionTreeClassifier(random_state=0)\n",
    "\n",
    "from sklearn import svm, grid_search, naive_bayes\n",
    "\n",
    "#SVC does not work\n",
    "#parameters = {'kernel':('linear', 'rbf'), 'C':[1, 20]}\n",
    "#svr = svm.SVC()\n",
    "#clf = grid_search.GridSearchCV(svr, parameters)\n",
    "\n",
    "#Bad results\n",
    "#clf= GaussianNB()\n",
    "\n",
    "#Works\n",
    "#from sklearn.neighbors import KNeighborsClassifier\n",
    "#clf = KNeighborsClassifier(n_neighbors=9)\n",
    "\n",
    "#Works\n",
    "from  sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_estimators=100,max_features=None)\n",
    "\n",
    "#Works\n",
    "#from  sklearn.ensemble import AdaBoostClassifier\n",
    "#clf = AdaBoostClassifier(n_estimators=100,random_state=None)\n",
    "\n",
    "#Works\n",
    "#from  sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "#clf = LinearDiscriminantAnalysis(solver='eigen', shrinkage=None, priors=None, n_components=None, store_covariance=False, tol=0.0001)\n",
    "\n",
    "#Works\n",
    "#from  sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "#clf = QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0, store_covariances=False, tol=0.0001)\n",
    "\n",
    "train_classifier(clf, X_train_cv, y_train_cv) # note: using entire training set here\n",
    "#print clf # you can inspect the learned model by printing it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following block and method is used to predict target for an input features set. This also returns the f_score based on the predicted target and the actual target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Predict on training set and compute F1 score\n",
    "from sklearn.metrics import f1_score\n",
    "def predict_labels(clf, features, target):\n",
    "    print \"Predicting labels using {}...\".format(clf.__class__.__name__)\n",
    "    start = time.time()\n",
    "    y_pred = clf.predict(features)\n",
    "    end = time.time()\n",
    "    print \"Done!\\nPrediction time (secs): {:.3f}\".format(end - start)\n",
    "    #return f1_score(target.values, y_pred, pos_label='yes')\n",
    "    return f1_score(target.values, y_pred) , (end - start)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following block of code is used to see the f1 score of the training data used as a test set and the cross-validation test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting labels using RandomForestClassifier...\n",
      "Done!\n",
      "Prediction time (secs): 1.165\n",
      "F1 score for training set: (0.58545795721584037, 1.1648800373077393)\n",
      "Predicting labels using RandomForestClassifier...\n",
      "Done!\n",
      "Prediction time (secs): 0.363"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akansha/anaconda2/lib/python2.7/site-packages/sklearn/metrics/classification.py:756: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "/home/akansha/anaconda2/lib/python2.7/site-packages/sklearn/metrics/classification.py:756: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "F1 score for test set: (0.58704401792242944, 0.3634068965911865)\n"
     ]
    }
   ],
   "source": [
    "#print y_train.head()\n",
    "#print X_train.head()\n",
    "train_cv_f1_score = predict_labels(clf, X_train_cv, y_train_cv)\n",
    "print \"F1 score for training set: {}\".format(train_cv_f1_score)\n",
    "# Predict on test data\n",
    "test_cv_f1_score = predict_labels(clf, X_test_cv, y_test_cv)\n",
    "print \"F1 score for test set: {}\".format(test_cv_f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfull!!\n"
     ]
    }
   ],
   "source": [
    "globalTestData = pd.read_csv(\"globalTest.csv\")\n",
    "#print globalTestData.head() # print the first 5 rows\n",
    "globalTestData=globalTestData.rename(columns = {' HotelCode':'HotelCode'})\n",
    "globalTestData=globalTestData.rename(columns = {' Age':'Age'})\n",
    "globalTestData=globalTestData.rename(columns = {' Gender':'Gender'})\n",
    "globalTestData=globalTestData.rename(columns = {' Number of Rooms':'Number of Rooms'})\n",
    "globalTestData=globalTestData.rename(columns = {' Check in date':'Check in date'})\n",
    "globalTestData=globalTestData.rename(columns = {' Check Out Date':'Check Out Date'})\n",
    "globalTestData=globalTestData.rename(columns = {' Seen Price':'Seen Price'})\n",
    "globalTestData=globalTestData.rename(columns = {' isClicked':'isClicked'})\n",
    "globalTestData=globalTestData.rename(columns = {' isBooked':'isBooked'})\n",
    "globalTestData=globalTestData.rename(columns = {' Segment':'Segment'})\n",
    "#print globalTestData.dtypes;\n",
    "globalTestData['Booking Date'] =  pd.to_datetime(globalTestData['Booking Date'])\n",
    "globalTestData['Check in date'] =  pd.to_datetime(globalTestData['Check in date'])\n",
    "globalTestData['Check Out Date'] =  pd.to_datetime(globalTestData['Check Out Date'])\n",
    "globalTestData['isClicked'] =  globalTestData['isClicked'].astype(str)\n",
    "globalTestData['isBooked'] =  globalTestData['isBooked'].astype(str)\n",
    "\n",
    "globalTestData['Stay Period'] = (globalTestData['Check Out Date'] - globalTestData['Check in date'])/np.timedelta64(1, 'D');\n",
    "globalTestData['Travel Gap'] = (globalTestData['Check in date'] - globalTestData['Booking Date'])/np.timedelta64(1, 'D');\n",
    "#print globalTestData.dtypes;\n",
    "#print \"Dataset has {} rows, {} columns\".format(*globalTestData.shape)\n",
    "#print globalTestData.head() # print the first 5 rows\n",
    "print \"Successfull!!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def string_to_date(my_string):\n",
    "    if '-' in my_string:\n",
    "        return datetime.datetime.strptime(my_string, '%d-%m-%y')\n",
    "    elif my_string.isdigit():\n",
    "        return datetime.datetime.strptime(my_string, '%d%m%y')\n",
    "    elif my_string.isalnum():\n",
    "        return datetime.datetime.strptime(my_string, '%d%b%Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Age\n",
      "0   38\n",
      "1   38\n",
      "2   38\n",
      "3   38\n",
      "4   38\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Gender'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-4cbf0b1d2c07>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mglobalTestData\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mX_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Gender'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Gender'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'male'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'female'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'isClicked'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'isClicked'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'True'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'False'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'isBooked'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'isBooked'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'True'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'False'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/akansha/anaconda2/lib/python2.7/site-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1990\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1991\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1992\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1993\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1994\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_getitem_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/akansha/anaconda2/lib/python2.7/site-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36m_getitem_column\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1997\u001b[0m         \u001b[1;31m# get column\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1998\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1999\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_item_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2000\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2001\u001b[0m         \u001b[1;31m# duplicate columns & possible reduce dimensionality\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/akansha/anaconda2/lib/python2.7/site-packages/pandas/core/generic.pyc\u001b[0m in \u001b[0;36m_get_item_cache\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m   1343\u001b[0m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1344\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1345\u001b[1;33m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1346\u001b[0m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_box_item_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1347\u001b[0m             \u001b[0mcache\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/akansha/anaconda2/lib/python2.7/site-packages/pandas/core/internals.pyc\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, item, fastpath)\u001b[0m\n\u001b[0;32m   3223\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3224\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misnull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3225\u001b[1;33m                 \u001b[0mloc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3226\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3227\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0misnull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/akansha/anaconda2/lib/python2.7/site-packages/pandas/indexes/base.pyc\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   1876\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1877\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1878\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1879\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1880\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/index.pyx\u001b[0m in \u001b[0;36mpandas.index.IndexEngine.get_loc (pandas/index.c:4027)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/index.pyx\u001b[0m in \u001b[0;36mpandas.index.IndexEngine.get_loc (pandas/index.c:3891)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/hashtable.pyx\u001b[0m in \u001b[0;36mpandas.hashtable.PyObjectHashTable.get_item (pandas/hashtable.c:12408)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/hashtable.pyx\u001b[0m in \u001b[0;36mpandas.hashtable.PyObjectHashTable.get_item (pandas/hashtable.c:12359)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Gender'"
     ]
    }
   ],
   "source": [
    "#Now separate training data\n",
    "#X_test=globalTestData.iloc[:,[3,4,5,8,9,10,16,17,18,19]];\n",
    "X_test=globalTestData.iloc[:,[3]];\n",
    "X_test['Gender'] = X_test['Gender'].replace(['male', 'female'], [1, 0])\n",
    "X_test['isClicked'] = X_test['isClicked'].replace(['True', 'False'], [1, 0])\n",
    "X_test['isBooked'] = X_test['isBooked'].replace(['True', 'False'], [1, 0])\n",
    "X_test[X_test < 0] = 0\n",
    "#print X_test.head();\n",
    "#print X_test.dtypes;\n",
    "#print X_train['Travel Gap'];\n",
    "print \"Successfull!!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training RandomForestClassifier...\n",
      "Done!\n",
      "Training time (secs): 5.341\n",
      "Successfull!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akansha/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:9: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n"
     ]
    }
   ],
   "source": [
    "# Fit model to training data\n",
    "#Best classifier\n",
    "#clf = DecisionTreeClassifier(random_state=0)\n",
    "train_classifier(clf, X_train, y_train) # note: using entire training set here\n",
    "y_pred = clf.predict(X_test)\n",
    "#print \"Dataset has {} rows, {} columns\".format(*y_pred.shape)\n",
    "#print y_pred.head()\n",
    "print \"Successfull!!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print y_pred\n",
    "#y_pred = y_pred.reshape((-1,1))\n",
    "#df_pred = pd.DataFrame({'Segment':y_pred[:,0]})\n",
    "#print df_pred.head()\n",
    "#print \"Dataset has {} rows, {} columns\".format(*df_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Search ID  Segment\n",
      "0    1000000        3\n",
      "1    1000001        3\n",
      "2    1000002        3\n",
      "3    1000003        3\n",
      "4    1000004        3\n",
      "Dataset has 297321 rows, 2 columns\n"
     ]
    }
   ],
   "source": [
    "y_pred = y_pred.reshape((-1,1))\n",
    "df_output= pd.DataFrame({'Search ID':globalTestData['Search ID'],'Segment':y_pred[:,0]})\n",
    "print df_output.head()\n",
    "print \"Dataset has {} rows, {} columns\".format(*df_output.shape)\n",
    "df_output.to_csv('akansha_out6.csv',columns=None, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
