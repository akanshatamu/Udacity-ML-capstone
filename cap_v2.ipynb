{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World!\n"
     ]
    }
   ],
   "source": [
    "print 'Hello World!'\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# Tell iPython to include plots inline in the notebook\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">>>Definition/Introduction/Motivation - Start<<<\n",
    "User clickstream data and information about a group of hotels is given. The users are classified into a set of classes.\n",
    "The classes are, Backpackers, Family, Couple. The objective is to predict the segment for a new customer using a\n",
    "classifier. The task is to explore different classifiers and then select the best one for predictions. Cross-validation\n",
    "is used to test the models by determining the F1 score.GraidCV and RandomCV are used to determine optimum\n",
    "hyper parameters for the classifiers.\n",
    ">>Data<<\n",
    "Data is given in the form of two tables.\n",
    ">train_search.csv:-<\n",
    "This table contains details about the hotel bookings. \n",
    ">Hotel.csv:-<  \n",
    "This table contains details about the hotel locations including a physical location and ratings\n",
    ">>Objectives<<\n",
    "1> Build a model than can predict the customer segment.\n",
    "2> Analyze different classifiers and understand their accuracy and speed.\n",
    "3> Implement CV based hyper-parameter optimization to tune the classifiers\n",
    ">>Metrics<<\n",
    "Cross-validation is used to validate the model for the test data against a model developed using the training data.\n",
    "The test and training data are randomly obtained from the \"train_search.csv\". An F1 score is obtained based on the\n",
    "predicted target and the acutal values for the target. This F1 score is the metrics for this project.\n",
    ">>Tasks<<\n",
    "The exercise is done  based on the following steps, \n",
    "<Step-1> Data mining and extraction:- This step involves reading the train_search.csv and hotel.csv and converting \n",
    "the data into a single table, globalTrain.csv. This process is executed using \"join\" using python pandas. \n",
    "However, here due to the size of data, I used mysql to generate the table. \n",
    "<Step-2> Identifying features and target in the test and training data. The features are extracted from the \n",
    "globalTrain and the target is the \"Segment\". This step involves the python pandas library and the following \n",
    "dataframes are created: X_train (features of trainigng data), y_train (target of training data), \n",
    "X_test (features of test dat i.e. evaluation.csv) and y_test (target of test data i.e. our final objective, \n",
    "the y_test is generated at the end). \n",
    "<Step-3> Performs cross-validation based F1 score tests and build the classifier models. Then fit the training \n",
    "data in the classifier. \n",
    ">>>Definition/Introduction/Motivation - End<<<"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">>>Step - 1 - Start<<<\n",
    "This step involved data extraction, mining and manipulations. Data is extracted and mined from the two inputted\n",
    "csv files.\n",
    ">>Data extraction and mining - Start<<\n",
    "This step involves the extraction of data and appluing necessary joins to converts multiple dataframes into a single\n",
    "dataframe.\n",
    "The primary tables are,\n",
    ">train_search.csv:-<\n",
    "Number of rows:- 162848\n",
    "Number of columns:- 12\n",
    "1> Search ID:- An unique number ... 2> Booking Date :- Hotel booking date ... 3> HotelCode:- A code for hotel \n",
    "identification. This is a foreign key mapped to the primary key in Hotel.csv ..4> Age :- Age of the customer ..\n",
    "5> Gender:- Gender of the customer .. 6> Number of Rooms :- Number of rooms booked in the hotel .. 7> Check in date :-\n",
    "Check in date in the hotel .. 8> Check out date :- Check out date for the booking .. 9> Seen Price :- Price for the \n",
    "booking ..10> isClicked:- click identifier on the website .. 11> isBooked :- a boolean value to see if it was booked\n",
    "online or not .. 12> Segment:- This is the classifier target.\n",
    ">Hotel.csv:-<  \n",
    "Number of rows:- 1000\n",
    "Number of columns:- 12    \n",
    "1> HotelCode:- An unique hote code identifier. This is the primary key in this table and used as a foreign key in\n",
    "train_search.csv... 2> City :- City in which the hotel is located 3> Latitude :- Latitude of the location of the hotel\n",
    "4> Longitude:- Longitude of the location of hotel 5> Star Rating:- Start rating of the hotel 6> TrioAdvisor Ration:- \n",
    "Trip advisor rating of the hotel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read train_search.csv and store the data in a data frame named, \"searchData\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfull!!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# Tell iPython to include plots inline in the notebook\n",
    "%matplotlib inline\n",
    "# Read dataset\n",
    "searchData = pd.read_csv(\"train_search.csv\")\n",
    "#print \"Dataset has {} rows, {} columns\".format(*searchData.shape)\n",
    "#print searchData.head() # print the first 5 rows\n",
    "print \"Successfull!!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read Hotel.csv and store the data in a data frame named, \"hotelData\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfull!!\n"
     ]
    }
   ],
   "source": [
    "# Read dataset\n",
    "hotelData = pd.read_csv(\"Hotel.csv\")\n",
    "#print \"Dataset has {} rows, {} columns\".format(*hotelData.shape)\n",
    "#print hotelData.head() # print the first 5 rows\n",
    "print \"Successfull!!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now join the two data frames based on \"HotelCode\" and develop a single data frame \"globalData\". in this process also create a csv file, \"globalTrain.csv\". Due to the size of searchData and computational limitations on my laptop I did not execute this join, rather used mysql to execute the join.\n",
    ">>Data extraction and mining - End<<"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#globalData = searchData.join(hotelData, on='HotelCode')\n",
    "#print \"Dataset has {} rows, {} columns\".format(*globalData.shape)\n",
    "#print globalData.head() # print the first 5 rows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">>Data manipulation - Start<<\n",
    "This step involves the following taskes,\n",
    "1> Removal of \"NA\" and missing data\n",
    "2> Convert strings to date time objects in the dataframes\n",
    "3> Addition of new columns by performing arithematic operations on the existing columns\n",
    "4> Convert discrete segments to discrete numbers understandable by the classifier. e.g. convert True/Falses to 1/0\n",
    "In the current data set following actions are performed,\n",
    "1> Replace True/False in \"isClicked\" column to 1/0\n",
    "2> Replace True/False in \"isBooked\" column to 1/0\n",
    "3> Replace 'backpacker'/'couple'/'family' in \"Segment\" to 1/3/2\n",
    "4> Convert \"Booking Date\" from string to datetime object\n",
    "5> Convert \"Check in date\" from string to datetime object\n",
    "6> Convert \"Check Out Date\" from string to datetime object\n",
    "7> Created a new column \"Stay Period\" - Difference in days between Check out date and Check in date\n",
    "8> Created a new column \"Travel Gap\" - Difference in days between booking date and check in date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfull!!\n"
     ]
    }
   ],
   "source": [
    "globalData = pd.read_csv(\"globalTrain.csv\")\n",
    "#print globalData.head() # print the first 5 rows\n",
    "globalData=globalData.rename(columns = {' HotelCode':'HotelCode'})\n",
    "globalData=globalData.rename(columns = {' Age':'Age'})\n",
    "globalData=globalData.rename(columns = {' Gender':'Gender'})\n",
    "globalData=globalData.rename(columns = {' Number of Rooms':'Number of Rooms'})\n",
    "globalData=globalData.rename(columns = {' Check in date':'Check in date'})\n",
    "globalData=globalData.rename(columns = {' Check Out Date':'Check Out Date'})\n",
    "globalData=globalData.rename(columns = {' Seen Price':'Seen Price'})\n",
    "globalData=globalData.rename(columns = {' isClicked':'isClicked'})\n",
    "globalData=globalData.rename(columns = {' isBooked':'isBooked'})\n",
    "globalData=globalData.rename(columns = {' Segment':'Segment'})\n",
    "#print globalData.dtypes;\n",
    "globalData['Booking Date'] =  pd.to_datetime(globalData['Booking Date'])\n",
    "globalData['Check in date'] =  pd.to_datetime(globalData['Check in date'])\n",
    "globalData['Check Out Date'] =  pd.to_datetime(globalData['Check Out Date'])\n",
    "globalData['isClicked'] =  globalData['isClicked'].astype(str)\n",
    "globalData['isBooked'] =  globalData['isBooked'].astype(str)\n",
    "\n",
    "globalData['Stay Period'] = (globalData['Check Out Date'] - globalData['Check in date'])/np.timedelta64(1, 'D');\n",
    "globalData['Travel Gap'] = (globalData['Check in date'] - globalData['Booking Date'])/np.timedelta64(1, 'D');\n",
    "#print globalData.dtypes;\n",
    "#print \"Dataset has {} rows, {} columns\".format(*globalData.shape)\n",
    "#print globalData.head() # print the first 5 rows\n",
    "print \"Successfull!!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfull!!\n"
     ]
    }
   ],
   "source": [
    "#Now separate training data - Target\n",
    "y_train=globalData.iloc[:,[11]];\n",
    "y_train['Segment'] = y_train['Segment'].replace(['backpacker', 'couple','family'], [1, 3, 2])\n",
    "#print y_train.head();\n",
    "#print y_train.dtypes;\n",
    "print \"Successfull!!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Age                     int64\n",
      "Gender                  int64\n",
      "Number of Rooms         int64\n",
      "Seen Price              int64\n",
      "isClicked               int64\n",
      "isBooked                int64\n",
      "Star Rating             int64\n",
      "TripAdvisor Rating    float64\n",
      "Stay Period           float64\n",
      "Travel Gap            float64\n",
      "dtype: object\n",
      "Successful!!\n"
     ]
    }
   ],
   "source": [
    "#Now separate training data\n",
    "X_train=globalData.iloc[:,[3,4,5,8,9,10,16,17,18,19]];\n",
    "#X_train=globalData.iloc[:,[3,5,8,16,18]];\n",
    "#X_train=globalData.iloc[:,[3,4,8,16,18]];\n",
    "#X_train=globalData.iloc[:,[3,4,18]];\n",
    "#X_train=globalData.iloc[:,[3,4,5,8,9,16,17,18,19]];\n",
    "X_train['Gender'] = X_train['Gender'].replace(['male', 'female'], [1, 0])\n",
    "X_train['isClicked'] = X_train['isClicked'].replace(['True', 'False'], [1, 0])\n",
    "X_train['isBooked'] = X_train['isBooked'].replace(['True', 'False'], [1, 0])\n",
    "X_train[X_train < 0] = 0\n",
    "#print X_train.head();\n",
    "print X_train.dtypes;\n",
    "#print X_train['Travel Gap'];\n",
    "print \"Successful!!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Age         Gender  Number of Rooms    Seen Price  \\\n",
      "count  162848.000000  162848.000000    162848.000000  1.628480e+05   \n",
      "mean       31.976512       0.508517         1.118626  2.887562e+04   \n",
      "std        14.679818       0.499929         0.442624  1.685715e+06   \n",
      "min        18.000000       0.000000         1.000000  0.000000e+00   \n",
      "25%        22.000000       0.000000         1.000000  8.600000e+03   \n",
      "50%        28.000000       1.000000         1.000000  1.240000e+04   \n",
      "75%        37.000000       1.000000         1.000000  1.770000e+04   \n",
      "max        80.000000       1.000000         8.000000  3.311511e+08   \n",
      "\n",
      "           isClicked       isBooked    Star Rating  TripAdvisor Rating  \\\n",
      "count  162848.000000  162848.000000  162848.000000       162848.000000   \n",
      "mean        0.598933       0.032736       3.810928            4.144279   \n",
      "std         0.490116       0.177945       0.730745            0.498001   \n",
      "min         0.000000       0.000000       1.000000            0.000000   \n",
      "25%         0.000000       0.000000       3.000000            4.000000   \n",
      "50%         1.000000       0.000000       4.000000            4.000000   \n",
      "75%         1.000000       0.000000       4.000000            4.500000   \n",
      "max         1.000000       1.000000       5.000000            5.000000   \n",
      "\n",
      "         Stay Period     Travel Gap  \n",
      "count  162848.000000  162848.000000  \n",
      "mean        2.523095      40.471169  \n",
      "std         1.953268      54.703405  \n",
      "min         1.000000       0.000000  \n",
      "25%         1.000000       5.251863  \n",
      "50%         2.000000      19.499039  \n",
      "75%         3.000000      53.033843  \n",
      "max        28.000000     479.734850  \n"
     ]
    }
   ],
   "source": [
    "print X_train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the X_train consists of the following 10 features,\n",
    "1> Age (3), \n",
    "2> Gender (4), \n",
    "3> Number of rooms (5), \n",
    "4> Seen price (8), \n",
    "5> isCLicked (9), \n",
    "6> isBooked (10), \n",
    "7> Star Rating (16), \n",
    "8> Trip Adviser Rating (17), \n",
    "9> Stay period (Difference in days between Check out date and Check in date) (18), \n",
    "10 > Travel Gap  (Difference in days between booking date and check in date) (19).\n",
    "Therefore, there are 10 features as mentioned above. Some of the features have a string/object value, convert those to discretes ones an zeros. \n",
    "*The number in braces is the column number in the \"globalData\" dataframe\n",
    ">>Data manipulation - End<<\n",
    ">>>Step - 1 - End<<<"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Now separate training data - Target\n",
    "#y_train=globalData.iloc[:,[11]];\n",
    "#y_train['Segment'] = y_train['Segment'].replace(['backpacker', 'couple','family'], [1, 3, 2])\n",
    "#print y_train.head();\n",
    "#print y_train.dtypes;\n",
    "#print \"Successfull!!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">>>Step - 2 - Start<<<\n",
    ">>Feature selection <<\n",
    "This step is very significant in the development of the classifier model. In this step a detailed analysis is done to\n",
    "determine the features those have a strong influence of the target. Intuitively it is logical that we need to build the\n",
    "classifier based on influential features to prevent over fitting. Through this analysis an exploration of different\n",
    "feature selection methods, such as KBest, Principal Component ANalysis (PCA) and Independent COmponent Analysis (ICA)\n",
    "are implemented.\n",
    "VarianceThreshold:-\n",
    "This method selects all the features those have a variance greater than thr threshold. In the current implementation\n",
    "we are removing the features those have a variance of less that 25% in data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1.80000000e+01   7.10000000e+03   3.00000000e+00   1.00000000e+00\n",
      "   0.00000000e+00]\n",
      "   Age  Gender  Number of Rooms  Seen Price  isClicked  isBooked  Star Rating  \\\n",
      "0   18       0                1        7100          1         0            3   \n",
      "\n",
      "   TripAdvisor Rating  Stay Period  Travel Gap  \n",
      "0                 3.5          1.0         0.0  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "sel = VarianceThreshold(threshold=(.5 * (1 - .5)))\n",
    "print sel.fit_transform(X_train)[0,:]\n",
    "print X_train.head(n=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the output, only the features those pass the threshold test are printed in the function call \"print sel.fit_transform(X_train)[0,:]\". From the variance threshold feature selection we find out FIVE features have a strong variance the follwing features are selected, 1> Age, 2> Seen Price 3> Star rating 4> Stay Period, 5> Travel Gap.\n",
    "Now let us build the features training data only with the five features in them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Age              int64\n",
      "Seen Price       int64\n",
      "Star Rating      int64\n",
      "Stay Period    float64\n",
      "Travel Gap     float64\n",
      "dtype: object\n",
      "Successful!!\n"
     ]
    }
   ],
   "source": [
    "#Now separate training data\n",
    "#X_train=globalData.iloc[:,[3,4,5,8,9,10,16,17,18,19]];\n",
    "X_train=globalData.iloc[:,[3,8,16,18,19]];\n",
    "#X_train['Gender'] = X_train['Gender'].replace(['male', 'female'], [1, 0])\n",
    "#X_train['isClicked'] = X_train['isClicked'].replace(['True', 'False'], [1, 0])\n",
    "#X_train['isBooked'] = X_train['isBooked'].replace(['True', 'False'], [1, 0])\n",
    "X_train[X_train < 0] = 0\n",
    "#print X_train.head();\n",
    "print X_train.dtypes;\n",
    "print \"Successful!!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now a correlation matrix is calcualted that determines the covarince between the parameters. It calculates the pearson's coefficient. The values range from -1 to 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfull!!\n",
      "                  Age  Seen Price  Star Rating  Stay Period  Travel Gap  \\\n",
      "Age          1.000000   -0.006204    -0.013152     0.046623    0.035981   \n",
      "Seen Price  -0.006204    1.000000     0.012070     0.008268   -0.002529   \n",
      "Star Rating -0.013152    0.012070     1.000000     0.025758    0.015049   \n",
      "Stay Period  0.046623    0.008268     0.025758     1.000000    0.294996   \n",
      "Travel Gap   0.035981   -0.002529     0.015049     0.294996    1.000000   \n",
      "Segment      0.075425   -0.002862     0.002235    -0.025639    0.018113   \n",
      "\n",
      "              Segment  \n",
      "Age          0.075425  \n",
      "Seen Price  -0.002862  \n",
      "Star Rating  0.002235  \n",
      "Stay Period -0.025639  \n",
      "Travel Gap   0.018113  \n",
      "Segment      1.000000  \n"
     ]
    }
   ],
   "source": [
    "bigdata = pd.concat([X_train, y_train], axis=1)\n",
    "print \"Successfull!!\"\n",
    "print bigdata.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the correlation matrix we observe the following, 1) Age is the most influential on the Segment as compared to other features. 2) Age is followed by \"Stay Period\" and \"Travel Gap\" in terms of covariances. 3) Star Rating is the least influential. Next we look at the principal components by performing a principal component analysis,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  5.40301043e-08  -1.00000000e+00  -5.23225422e-09  -9.57986790e-09\n",
      "    8.20738300e-08]\n",
      " [ -1.04019883e-02  -8.25247335e-08  -2.01053042e-04  -1.05488136e-02\n",
      "   -9.99890234e-01]\n",
      " [  9.99933750e-01   5.31264886e-08  -6.82001479e-04   4.77206197e-03\n",
      "   -1.04526490e-02]\n",
      " [  4.87437946e-03   1.07588893e-08  -1.05208931e-02  -9.99877642e-01\n",
      "    1.05000874e-02]\n",
      " [ -7.31188424e-04   5.09970414e-09  -9.99944401e-01   1.05190570e-02\n",
      "    9.76948294e-05]]\n",
      "[  9.99999999e-01   1.05329430e-09   7.57287359e-11   1.22388248e-12\n",
      "   1.87696352e-13]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Annotation at 0x7f8af9ea2510>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe4AAAFXCAYAAABk0q3rAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nMAjBDQCAQQhuAAAMQnADAGCQ/w/XbfYqLPgeEwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8afb530b50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=5)\n",
    "pca.fit(X_train)\n",
    "# Print the components and the amount of variance in the data contained in each dimension\n",
    "print pca.components_\n",
    "print pca.explained_variance_ratio_\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(pca.components_[0],pca.components_[1])\n",
    "ax.annotate('Age', (pca.components_[0][0],pca.components_[1][0]))\n",
    "ax.annotate('Seen Price', (pca.components_[0][1],pca.components_[1][1]))\n",
    "ax.annotate('Star Rating', (pca.components_[0][2],pca.components_[1][2]))\n",
    "ax.annotate('Stay Period', (pca.components_[0][3],pca.components_[1][3]))\n",
    "ax.annotate('Travel Gap', (pca.components_[0][4],pca.components_[1][4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the principal components we observe that only one component (1st component) accounts for 99% of variance in data. From the plot it is visisble that Seen Price, Travel Gap and (Age, Start Rating, Stay Period) are far apart from each other. Therefore each of these components can be individually used for building the predictive model. Now let us look at the independent component by performing and independent component analysys (ICA)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -8.32425274e-06  -1.36867712e-11  -2.19274693e-04   1.32039370e-03\n",
      "   -8.99445372e-06]\n",
      " [  1.68740925e-04   8.82687523e-12  -4.26115941e-05   9.23210461e-06\n",
      "   -2.56443059e-07]\n",
      " [  4.72867816e-06  -6.30755064e-12   9.34126252e-05   1.42665128e-04\n",
      "   -4.65621094e-05]\n",
      " [ -3.46457503e-08  -1.47005666e-09   7.80534108e-06  -8.66909533e-07\n",
      "    3.49387021e-08]\n",
      " [ -3.90965707e-06   1.46184409e-11  -3.38429291e-03  -5.14362096e-05\n",
      "   -3.09333401e-07]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f8af9e812d0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtIAAAFuCAYAAACsiv85AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\npAEAAAAD/w9S1HYvUkF9qwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8afb530990>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.decomposition import FastICA\n",
    "ica = FastICA(n_components=5, random_state=1)\n",
    "ica.fit(X_train)\n",
    "# Print the independent components\n",
    "print ica.components_\n",
    "pd.DataFrame(ica.components_, columns=X_train.columns).plot(kind = 'bar', figsize = (12, 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ICA converts a multivariate dataset to additive individual components. The resulting matrix\n",
    "above is the ICA components. Each vector corresponds to a component, and it defines the independence\n",
    "with respect to other features. These components help in projecting the data from ICA axis to the original\n",
    "axis. The row feature can be reconstructed by a linear combination of the columns in that specific row. AT this point based on PCA and ICA, the features selected are, \"Age\", \"Star ating\" and \"Travel Gap\". Let us update our training data to contain only these three features,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Age              int64\n",
      "Star Rating      int64\n",
      "Travel Gap     float64\n",
      "dtype: object\n",
      "Successful!!\n"
     ]
    }
   ],
   "source": [
    "X_train=globalData.iloc[:,[3,16,19]];\n",
    "X_train[X_train < 0] = 0\n",
    "#print X_train.head();\n",
    "print X_train.dtypes;\n",
    "print \"Successful!!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">>Feature selection - End <<\n",
    ">>>Step - 2 - End<<<"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">>> Step -3 - Start <<<\n",
    ">> Build testing model <<\n",
    "The best way to test a classifier model is cross-validation. In cross-validation an input set is brocken into training and testing set. The testting set is used to test the model built using the training set. And then the F1 score is computed. The next block of code is used to split the input data randomly into traing set and testing set. After the block is executed a set of training and test features and target are available for cross validation. The variables generated are X_train_cv, y_train_cv, X_test_cv and y_test_cv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 122848 samples\n",
      "Test set: 40000 samples\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "# First, decide how many training vs test samples you want\n",
    "num_test = 40000\n",
    "X_train_cv, X_test_cv, y_train_cv, y_test_cv = train_test_split(X_train, y_train, test_size=num_test)\n",
    "print \"Training set: {} samples\".format(X_train_cv.shape[0])\n",
    "print \"Test set: {} samples\".format(X_test_cv.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next block of code is a method that fits a training data into an inputted classifier and also returns and prints the time it takes to train the model. Create a method, \"train_classifier\" that trains an input classifier with inputted training data. The mthods prints the training time and also returns the time to the caller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time as time\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "def train_classifier(clf, X_train, y_train):\n",
    "    #print \"Training {}...\".format(clf.__class__.__name__)\n",
    "    start = time.time()\n",
    "    clf.fit(X_train, y_train)\n",
    "    end = time.time()\n",
    "    #print \"Done!\\nTraining time (secs): {:.3f}\".format(end - start)\n",
    "    return (end - start)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try different classifiers and track the time and the accuracy. In the following code snippet, several different types of classifiers are instantiated and each of their's prediction accurace is determined useing a brute force method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A decision tree classifier is initiated and a classifier model is built and fittedwith the cross-validation training features and target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfull!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akansha/anaconda2/lib/python2.7/site-packages/sklearn/utils/validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "# Fit model to training data\n",
    "#Best classifier\n",
    "import time as time\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import svm, grid_search, naive_bayes\n",
    "from sklearn.grid_search import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn import linear_model\n",
    "from  sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "\n",
    "#clf = DecisionTreeClassifier(random_state=0)\n",
    "#clf = DecisionTreeClassifier(min_samples_split= 2, max_leaf_nodes= 32, criterion= 'entropy',\n",
    "#                             max_depth = 9, min_samples_leaf=179)\n",
    "#clf = DecisionTreeClassifier(min_samples_split= 21, max_leaf_nodes= 20, criterion= 'gini',\n",
    "#                             max_depth = None, min_samples_leaf=11)\n",
    "\n",
    "#dtr_params = {'criterion':(\"gini\",\"entropy\")}\n",
    "#dtc2 = DecisionTreeClassifier(random_state=0)\n",
    "#clf = linear_model.LogisticRegression()\n",
    "#clf = grid_search.GridSearchCV(dtc2, dtr_params)\n",
    "\n",
    "#dtr_params = {'criterion':(\"gini\",\"entropy\"),'presort':(\"True\",\"False\"),\n",
    "#              'min_weight_fraction_leaf':(0,0.25,0.5), 'min_samples_leaf':(1,2,3),\n",
    "#              'min_samples_split':(2,4,8,16,32),'min_samples_split':(2,4,8,16), \n",
    "#              'max_features':(\"auto\",\"sqrt\",\"log2\"),'max_depth':np.arange(1,5,1)}\n",
    "#dtc2 = DecisionTreeClassifier(random_state=0)\n",
    "#clf = grid_search.GridSearchCV(dtc2, dtr_params)   \n",
    "\n",
    "#from  sklearn.ensemble import RandomForestClassifier\n",
    "#clf = RandomForestClassifier()\n",
    "#clf1 = RandomForestClassifier(n_estimators=20)\n",
    "# use a full grid over all parameters\n",
    "#param_grid = {#\"max_depth\": [3, None],\n",
    "              #\"max_features\": [1, 3, 10],\n",
    "              #\"min_samples_split\": [1, 3, 10],\n",
    "              #\"min_samples_leaf\": [1, 3, 10],\n",
    "              #\"bootstrap\": [True, False],\n",
    "              # \"criterion\": [\"gini\", \"entropy\"]\n",
    "              #  \"n_estimators\": [5, 20,30]}\n",
    "#clf = GridSearchCV(clf1, param_grid=param_grid)\n",
    "\n",
    "#parameters={'C' : [.005,.05,.5,1.,10.,100.,],\n",
    "#'fit_intercept' : [True, False],\n",
    "#'class_weight': [ None,'balanced'],\n",
    "#'random_state' : [None,42],\n",
    "#'penalty': ['l1', 'l2']\n",
    "#}\n",
    "#clf = svm.SVC()\n",
    "\n",
    "#SVC does not work\n",
    "#parameters = {'kernel':('linear', 'rbf'), 'C':[1, 20]}\n",
    "#svr = svm.SVC()\n",
    "#clf = SVC(kernel=\"linear\", C=1.0)\n",
    "\n",
    "#Bad results\n",
    "#clf= GaussianNB()\n",
    "\n",
    "#clf=SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
    "#    decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',\n",
    "#    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
    "#    tol=0.001, verbose=False)\n",
    "\n",
    "#from sklearn.linear_model import LogisticRegression\n",
    "#clf = LogisticRegression()\n",
    "\n",
    "\n",
    "#from sklearn.linear_model import SGDRegressor\n",
    "#clf = SGDRegressor(alpha=0.0001, average=False, epsilon=0.1, eta0=0.01,\n",
    "#             fit_intercept=True, l1_ratio=0.15, learning_rate='invscaling',\n",
    "#             loss='squared_loss', n_iter=5, penalty='l2', power_t=0.25,\n",
    "#             random_state=None, shuffle=True, verbose=0, warm_start=False)\n",
    "\n",
    "#from sklearn.naive_bayes import BernoulliNB\n",
    "#clf = BernoulliNB()\n",
    "\n",
    "#from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "clf=PassiveAggressiveClassifier()\n",
    "\n",
    "#Works\n",
    "#from sklearn.neighbors import KNeighborsClassifier\n",
    "#clf = KNeighborsClassifier(n_neighbors=35)\n",
    "\n",
    "#Works\n",
    "#from  sklearn.ensemble import RandomForestClassifier\n",
    "#clf = RandomForestClassifier(criterion='entropy',n_estimators=100, max_features=None,random_state=None)\n",
    "#clf = RandomForestClassifier(n_estimators=20,min_samples_split=4)\n",
    "\n",
    "\n",
    "#Works\n",
    "#from  sklearn.ensemble import RandomForestClassifier\n",
    "#clf = RandomForestClassifier(n_estimators=100)\n",
    "#clf = RandomForestClassifier(n_estimators=100,max_features=None)\n",
    "\n",
    "#Works\n",
    "#from  sklearn.ensemble import AdaBoostClassifier\n",
    "#clf = AdaBoostClassifier(n_estimators=50)\n",
    "#clf = AdaBoostClassifier(n_estimators=60,learning_rate=0.65)\n",
    "\n",
    "#from sklearn.tree import DecisionTreeClassifier\n",
    "#clf2 = DecisionTreeClassifier(min_samples_split= 2, max_leaf_nodes= 20, criterion= 'gini',\n",
    "#                             max_depth = None, min_samples_leaf=1)\n",
    "#clf = AdaBoostClassifier(base_estimator=clf2,n_estimators=1,learning_rate=18,algorithm='SAMME')\n",
    "\n",
    "\n",
    "#clf1 = AdaBoostClassifier()\n",
    "#param_grid = { \"n_estimators\": [5, 20,30]}\n",
    "#lf = GridSearchCV(clf1, param_grid=param_grid)\n",
    "\n",
    "#from  sklearn.ensemble import RandomForestClassifier\n",
    "#clf1 = RandomForestClassifier(n_estimators=20)\n",
    "#clf = AdaBoostClassifier(base_estimator=clf1,n_estimators=50)\n",
    "\n",
    "#Works\n",
    "#from  sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "#clf = LinearDiscriminantAnalysis(solver='eigen', shrinkage='auto', tol=0.0001)\n",
    "\n",
    "#Works\n",
    "#from  sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "#clf = QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0, store_covariances=False, tol=0.000000001)\n",
    "\n",
    "train_classifier(clf, X_train_cv, y_train_cv) # note: using entire training set here\n",
    "#print clf # you can inspect the learned model by printing it\n",
    "\n",
    "print \"Successfull!!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Predict on training set and compute F1 score\n",
    "from sklearn.metrics import f1_score\n",
    "def predict_labels(clf, features, target):\n",
    "    #print \"Predicting labels using {}...\".format(clf.__class__.__name__)\n",
    "    start = time.time()\n",
    "    y_pred = clf.predict(features)\n",
    "    end = time.time()\n",
    "    #print \"Done!\\nPrediction time (secs): {:.3f}\".format(end - start)\n",
    "    #return f1_score(target.values, y_pred, pos_label='yes')\n",
    "    return f1_score(target.values, y_pred) , (end - start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akansha/anaconda2/lib/python2.7/site-packages/sklearn/metrics/classification.py:756: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "/home/akansha/anaconda2/lib/python2.7/site-packages/sklearn/metrics/classification.py:756: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n"
     ]
    }
   ],
   "source": [
    "#print y_train.head()\n",
    "#print X_train.head()\n",
    "train_cv_f1_score = predict_labels(clf, X_train_cv, y_train_cv)\n",
    "#print \"F1 score for training set: {}\".format(train_cv_f1_score)\n",
    "# Predict on test data\n",
    "test_cv_f1_score = predict_labels(clf, X_test_cv, y_test_cv)\n",
    "#print \"F1 score for test set: {}\".format(test_cv_f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Report\n",
    "from astropy.table import Table, Column\n",
    "import time as time\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import svm, grid_search, naive_bayes\n",
    "from sklearn.grid_search import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn import linear_model\n",
    "from  sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from  sklearn.ensemble import AdaBoostClassifier\n",
    "from  sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "&lt;Table length=10&gt;\n",
       "<table id=\"table139673253914448\">\n",
       "<thead><tr><th>Type</th><th>Training time</th><th>Prediction time-Training</th><th>F1 score-Training</th><th>Prediction time-Testing</th><th>F1 score-Testing</th></tr></thead>\n",
       "<thead><tr><th>str25</th><th>float32</th><th>float32</th><th>float32</th><th>float32</th><th>float32</th></tr></thead>\n",
       "<tr><td>PassiveAggressiveClassifi</td><td>0.00794792</td><td>0.00794792</td><td>0.26596</td><td>0.00275993</td><td>0.26858</td></tr>\n",
       "<tr><td>GaussianNB</td><td>0.0181749</td><td>0.0181749</td><td>0.587897</td><td>0.00532699</td><td>0.589002</td></tr>\n",
       "<tr><td>DecisionTreeClassifier</td><td>0.0221438</td><td>0.0221438</td><td>0.986583</td><td>0.007617</td><td>0.977333</td></tr>\n",
       "<tr><td>RandomForestClassifier</td><td>0.290855</td><td>0.290855</td><td>0.983003</td><td>0.095232</td><td>0.952203</td></tr>\n",
       "<tr><td>BernoulliNB</td><td>0.015748</td><td>0.015748</td><td>0.323137</td><td>0.00455809</td><td>0.321842</td></tr>\n",
       "<tr><td>LogisticRegression</td><td>0.00847602</td><td>0.00847602</td><td>0.479718</td><td>0.00287604</td><td>0.477646</td></tr>\n",
       "<tr><td>KNeighborsClassifier</td><td>0.880349</td><td>0.880349</td><td>0.940098</td><td>0.286511</td><td>0.909916</td></tr>\n",
       "<tr><td>AdaBoostClassifier</td><td>1.43477</td><td>1.43477</td><td>0.601057</td><td>0.459271</td><td>0.598315</td></tr>\n",
       "<tr><td>LinearDiscriminantAnalysi</td><td>0.00837684</td><td>0.00837684</td><td>0.47976</td><td>0.00288296</td><td>0.477299</td></tr>\n",
       "<tr><td>QuadraticDiscriminantAnal</td><td>0.0278611</td><td>0.0278611</td><td>0.587528</td><td>0.00824785</td><td>0.588294</td></tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Table length=10>\n",
       "           Type           Training time ... F1 score-Testing\n",
       "          str25              float32    ...     float32     \n",
       "------------------------- ------------- ... ----------------\n",
       "PassiveAggressiveClassifi    0.00794792 ...          0.26858\n",
       "               GaussianNB     0.0181749 ...         0.589002\n",
       "   DecisionTreeClassifier     0.0221438 ...         0.977333\n",
       "   RandomForestClassifier      0.290855 ...         0.952203\n",
       "              BernoulliNB      0.015748 ...         0.321842\n",
       "       LogisticRegression    0.00847602 ...         0.477646\n",
       "     KNeighborsClassifier      0.880349 ...         0.909916\n",
       "       AdaBoostClassifier       1.43477 ...         0.598315\n",
       "LinearDiscriminantAnalysi    0.00837684 ...         0.477299\n",
       "QuadraticDiscriminantAnal     0.0278611 ...         0.588294"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noOfTypes =10;\n",
    "t1 = Table(names=('Type','Training time' ,'Prediction time-Training', 'F1 score-Training', \n",
    "                  'Prediction time-Testing', 'F1 score-Testing'), dtype=('S25', 'f4','f4', 'f4','f4', 'f4'))\n",
    "\n",
    "for i in range(0,noOfTypes):\n",
    "    if i==0:\n",
    "        clf=PassiveAggressiveClassifier();\n",
    "    if i==1:\n",
    "        clf=GaussianNB(); \n",
    "    if i==2:    \n",
    "        clf = DecisionTreeClassifier(random_state=0)\n",
    "    if i==3:    \n",
    "        clf = RandomForestClassifier()\n",
    "    if i==4:    \n",
    "        clf = BernoulliNB()\n",
    "    if i==5:    \n",
    "        clf = LogisticRegression()\n",
    "    if i==6:    \n",
    "        clf = KNeighborsClassifier()\n",
    "    if i==7:    \n",
    "        clf = AdaBoostClassifier()\n",
    "    if i==8:    \n",
    "        clf = LinearDiscriminantAnalysis()\n",
    "    if i==9:    \n",
    "        clf = QuadraticDiscriminantAnalysis()\n",
    "        \n",
    "    clfType=clf.__class__.__name__;\n",
    "    trainTime=train_classifier(clf, X_train_cv, y_train_cv);\n",
    "    retVec=predict_labels(clf, X_train_cv, y_train_cv)\n",
    "    trainF1Score=retVec[0];\n",
    "    trainTime=retVec[1];\n",
    "    retVec1=predict_labels(clf, X_test_cv, y_test_cv)\n",
    "    testF1Score=retVec1[0];\n",
    "    testTime=retVec1[1];\n",
    "    t1.add_row((clfType,trainTime,trainTime, trainF1Score,testTime, testF1Score))\n",
    "\n",
    "#t1 = Table(names=('Type','Training time' ,'Prediction time-Training', 'F1 score-Training', 'Prediction time-Testing', 'F1 score-Testing'), dtype=('S13', 'f4','f4', 'f4','f4', 'f4'))\n",
    "#t1.add_row(('SVC',1,100, 3,4, 5))\n",
    "#t1.add_row(('SVC',2, 200, 33,44, 55))\n",
    "\n",
    "t1    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above table presents the performance analysis of a set of 10 different classifiers. The table presents the training time, prediction time of training data, f1 score of training data, prediction time of test data and f1 score os test data. Let us select the BEST THREE classifiers for further analysis. The next step is to use grid and random CV to determine optimizaed hyper parameters. The classifiers chosen at this point are,\n",
    "1> Decision Tree Classifier\n",
    "2> Random Forst Classifier\n",
    "3> KNeighbors Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the search of hyper parameters, the idea is to perform grid based search and random search for all the three classifier types and the analysis is plotted. First, few methods are implemeted,\n",
    "run_gridsearch :- This functions takes the data, parameters, number of folds and the classifier as input. It runs different several parameter options based on a grid and returns the best parameter set.\n",
    "run_randomsearch :- This functions takes the data, parameters, number of folds and the classifier as input. It runs different several parameter options randomly and returns the best parameter set.\n",
    "report :- Reports the top three scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "def run_gridsearch(X, y, clf, param_grid, cv=5):\n",
    "    \"\"\"\"\"\n",
    "    Args\n",
    "    ----\n",
    "    X -- features\n",
    "    y -- targets (classes)\n",
    "    cf -- scikit-learn classifier\n",
    "    param_grid -- [dict] parameter settings to test\n",
    "    cv -- fold of cross-validation, default 5\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    top_params -- [dict] from report()\n",
    "    \"\"\"\n",
    "    grid_search = GridSearchCV(clf,\n",
    "                               param_grid=param_grid,\n",
    "                               cv=cv, verbose=5)\n",
    "    start = time()\n",
    "    grid_search.fit(X, y)\n",
    "\n",
    "    #print((\"\\nGridSearchCV took {:.2f} \"\n",
    "    #       \"seconds for {:d} candidate \"\n",
    "    #       \"parameter settings.\").format(time() - start,\n",
    "    #            len(grid_search.grid_scores_)))\n",
    "\n",
    "    top_params = report(grid_search.grid_scores_, 1)\n",
    "    return  top_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "def run_randomsearch(X, y, clf, para_dist, cv=5,\n",
    "                     n_iter_search=20):\n",
    "    \"\"\"\n",
    "\n",
    "    Args\n",
    "    ----\n",
    "    X -- features\n",
    "    y -- targets (classes)\n",
    "    cf -- scikit-learn classifier\n",
    "    param_dist -- [dict] list, distributions of parameters\n",
    "                  to sample\n",
    "    cv -- fold of cross-validation, default 5\n",
    "    n_iter_search -- number of random parameter sets to try,\n",
    "                     default 20.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    top_params -- [dict] from report()\n",
    "    \"\"\"\n",
    "    random_search = RandomizedSearchCV(clf,\n",
    "                        param_distributions=param_dist,\n",
    "                        n_iter=n_iter_search, verbose=5)\n",
    "\n",
    "    start = time()\n",
    "    random_search.fit(X, y)\n",
    "    #print((\"\\nRandomizedSearchCV took {:.2f} seconds \"\n",
    "    #       \"for {:d} candidates parameter \"\n",
    "    #       \"settings.\").format((time() - start),\n",
    "    #                          n_iter_search))\n",
    "\n",
    "    top_params = report(random_search.grid_scores_, 1)\n",
    "    return  top_params\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "from operator import itemgetter\n",
    "from scipy.stats import randint\n",
    "def report(grid_scores, n_top=3):\n",
    "    \"\"\"Report top n_top parameters settings, default n_top=3.\n",
    "\n",
    "    Args\n",
    "    ----\n",
    "    grid_scores -- output from grid or random search\n",
    "    n_top -- how many to report, of top models\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    top_params -- [dict] top parameter settings found in\n",
    "                  search\n",
    "    \"\"\"\n",
    "    top_scores = sorted(grid_scores,\n",
    "                        key=itemgetter(1),\n",
    "                        reverse=True)[:n_top]\n",
    "    for i, score in enumerate(top_scores):\n",
    "        print(\"Model with rank: {0}\".format(i + 1))\n",
    "        print((\"Mean validation score: \"\n",
    "               \"{0:.3f} (std: {1:.3f})\").format(\n",
    "               score.mean_validation_score,\n",
    "               np.std(score.cv_validation_scores)))\n",
    "        print(\"Parameters: {0}\".format(score.parameters))\n",
    "        print(\"\")\n",
    "\n",
    "    return top_scores[0].parameters\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.stats import randint as sp_randint\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import svm, grid_search, naive_bayes\n",
    "from sklearn.grid_search import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn import linear_model\n",
    "#print(\"-- Grid Parameter Search via 10-fold CV\")\n",
    "\n",
    "#Decision Tree Classifier \n",
    "#parameter distribution for Grid CV\n",
    "#param_dist = {\"criterion\": [\"gini\", \"entropy\"],\n",
    "#              \"min_samples_split\": [2,10,20],\n",
    "#              \"max_depth\": [None, 2, 5, 10],\n",
    "#              \"min_samples_leaf\": [1,5,10],\n",
    "#              \"max_leaf_nodes\": [None, 5, 10, 20],\n",
    "#              }\n",
    "#dt = DecisionTreeClassifier()\n",
    "\n",
    "#parameter distribution for random CV\n",
    "#param_dist = {\"criterion\": [\"gini\", \"entropy\"],\n",
    "#              \"min_samples_split\": sp_randint(1, 100),\n",
    "#              \"max_depth\": sp_randint(1, 20),\n",
    "#              \"min_samples_leaf\": sp_randint(1, 200),\n",
    "#              \"max_leaf_nodes\": sp_randint(2, 100),              \n",
    "#              }\n",
    "#dt = DecisionTreeClassifier()\n",
    "\n",
    "\n",
    "\n",
    "#RandomForestClassifier\n",
    "# build a classifier\n",
    "# Random parameters\n",
    "#param_dist = {\"max_depth\": [3, None],\n",
    "#              \"max_features\": sp_randint(1, 4),\n",
    "#              \"min_samples_split\": sp_randint(1, 11),\n",
    "#              \"min_samples_leaf\": sp_randint(1, 11),\n",
    "#              \"bootstrap\": [True, False],\n",
    "#              \"criterion\": [\"gini\", \"entropy\"]}\n",
    "#dt = RandomForestClassifier()\n",
    "\n",
    "# Grid parameters\n",
    "#param_dist = {\"max_depth\": [3, None],\n",
    "#              \"max_features\": [1,2,4],\n",
    "#              \"min_samples_split\": [2,5,11],\n",
    "#              \"min_samples_leaf\":  [2,5,11],\n",
    "#              \"bootstrap\": [True, False],\n",
    "#              \"criterion\": [\"gini\", \"entropy\"]}\n",
    "#dt = RandomForestClassifier()\n",
    "\n",
    "\n",
    "#KNearest Neighbors\n",
    "#GridCV parameters\n",
    "#param_dist = {\"n_neighbors\": [1,5,20,100],\n",
    "#              \"leaf_size\": [5,20,100],\n",
    "#              \"p\": [1,2,10],\n",
    "#              \"algorithm\": [\"auto\", \"ball_tree\",\"kd_tree\",\"brute\"],\n",
    "#              \"weights\": [\"uniform\", \"distance\"]}\n",
    "#dt = KNeighborsClassifier()\n",
    "\n",
    "#RandomCV parameters\n",
    "#param_dist = {\"n_neighbors\": sp_randint(2, 100),\n",
    "#              \"leaf_size\": sp_randint(2, 100),\n",
    "#              \"p\": sp_randint(2, 10),\n",
    "#              \"algorithm\": [\"auto\", \"ball_tree\",\"kd_tree\"],\n",
    "#              \"weights\": [\"uniform\", \"distance\"]}\n",
    "#dt = KNeighborsClassifier()\n",
    "\n",
    "\n",
    "features = [\"Age\",\"Star Rating\",\"Travel Gap\"]\n",
    "\n",
    "y_arr = y_train[\"Segment\"]\n",
    "X_arr = X_train[features]\n",
    "#Call grid based CV search\n",
    "#ts_gs = run_gridsearch(X_arr, y_arr, dt, param_dist, cv=10)\n",
    "#Call random CV based search\n",
    "#ts_rs = run_randomsearch(X_arr, y_arr, dt, param_dist, cv=10,n_iter_search=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results fo CV based hyper-parameter search:-\n",
    "\n",
    "DecisionTreeClassifier - RandomCV :-\n",
    "Model with rank: 1\n",
    "Mean validation score: 0.589 (std: 0.005)\n",
    "Parameters: {'min_samples_split': 95, 'max_leaf_nodes': 54, 'criterion': 'entropy', 'max_depth': 11, 'min_samples_leaf': 42}\n",
    "DecisionTreeClassifier - GridCV :-\n",
    "Model with rank: 1\n",
    "Mean validation score: 0.609003 (std: 0.005)\n",
    "Parameters: {'min_samples_split': 10, 'max_leaf_nodes': 10, 'criterion': 'gini', 'max_depth': 'None', 'min_samples_leaf': 5}\n",
    "\n",
    "RandomForest - RandomCV :-\n",
    "Model with rank: 1\n",
    "Mean validation score: 0.589 (std: 0.003)\n",
    "Parameters: {'bootstrap': True, 'min_samples_leaf': 9, 'min_samples_split': 8, 'criterion': 'entropy', 'max_features': 3, 'max_depth': 3}\n",
    "RandomForest - GridCV :-\n",
    "Model with rank: 1\n",
    "Mean validation score: 0.590211 (std: 0.004)\n",
    "Parameters: {'bootstrap': True, 'min_samples_leaf': 2, 'min_samples_split': 5, 'criterion': 'gini', 'max_features': 1, 'max_depth': 3}\n",
    "\n",
    "KNearestNeighbors - RandomCV :-\n",
    "Model with rank: 1\n",
    "Mean validation score: 0.549 (std: 0.004)\n",
    "Parameters: {'n_neighbors': 86, 'weights': 'uniform', 'leaf_size': 93, 'algorithm': 'kd_tree', 'p': 5}\n",
    "KNearestNeighbors - GridCV :-\n",
    "Model with rank: 1\n",
    "Mean validation score: 0.542189 (std: 0.004)\n",
    "Parameters: {'n_neighbors': 1, 'weights': 'uniform', 'leaf_size': 5, 'algorithm': 'kd_tree', 'p': 10}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "From the results the DecisionTreeClassifier with the following parameters is the best. \n",
    "('min_samples_split': 10, 'max_leaf_nodes': 10, 'criterion': 'gini', 'max_depth': 'None', 'min_samples_leaf': 5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
